{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f414bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18516e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=J5_-l7WIO_w\",\n",
    "    language=[\"en\", \"hi\"],\n",
    "    translation=\"en\"\n",
    ")\n",
    "\n",
    "transcript=loader.load()\n",
    "# print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi guys, my name is Nitesh and you are welcome to my YouTube channel.  In this video also we will continue our lang chain playlist. Uh in the last video, we started studying rag and we focused on discussing the theory around rag.  Did I tell you what rags are?  Why is it needed ?  I also demonstrated comparing racks there using techniques like fine tuning.  And today's video is a continuation of the previous video. Where we will practically create a rag based system using lang chain.  The plan is that I will take a problem statement and create a rig based system around that problem statement and we are going to do all this code in lang chain. So whatever you have read till now in the previous four videos, document loaders, text splitters, vector strings, we will use all these in today's video and using these, we will create a rag base system.  On the whole it's going to be a very interesting video.  Let's start.  So come on guys, first let's talk about the problem statement.  What are we going to make in this video?  So we will work on a very simple yet very important problem statement.  And the name of our problem statement is YouTube Chat.  I don't know may be YouTube chat could be a good name.  So basically what we are going to do is we are going to create a rag base system with the help of which you can chat with any YouTube video in real time. Ok?  So we all watch videos on YouTube.  Right?  Some videos are very long.  Like especially if you are watching any podcast etc. then it is two to three hours in length.   The problem is that if you want to understand the entire content of that video then you will have to watch the entire video.  Our solution solves this problem for you. For example, let’s say I’m watching a podcast which is 3 hours long.   Let me assume, there was a question about that podcast, whether AI is being discussed in this podcast ?  So, I will quickly put a question in our system and my rack based system will quickly reply to me that yes, in this particular video, AI has been discussed and these are key pointers.  Ok?   Can I quickly ask you if you can summarize this entire video in five bullet points?  So, this rack base system of mine will instantly create a summary of the entire video and give it to me.  Tomorrow, if you are watching a lecture on data science and suddenly you have a doubt in some part, then you will quickly enter that doubt in this system and this system will solve that doubt for you. So basically this is a chat system with the help of which you can ask whatever you want about any YouTube video. So I am pretty sure you all will be able to relate to what problem this system is solving. And that is why I thought this could be a very good problem statement for us.  This system which will be created can be created in many ways. Meaning the final product can look like anything.  One option is that we create a Chrome plugin.  Where the user will install this Chrome plugin. And when the video is playing, click here and a chat interface will open and here the user can chat while watching the video.  So this is the best solution if you can make it.  But for this you will need knowledge of HTML, CSS and JavaScript.  The second option is if you do n’t know HTML, CSS, JavaScript then what you can do is you can create a Streamlet website where you will paste the link of the YouTube video here and as soon as you click on submit, a chat window will open in a new window and here you can chat about that video. So for this you do not need to know HTML, CSS, JavaScript but you should know how to work with tools like Streamlet. Ok?  A but today we are going to focus on rag so I will not focus too much on UI right now.  I would focus more on functionality. And that is why whatever we create will be created inside a Google Collab Notebook. But I would recommend that once you try and run this project in Google Collab, then definitely you should try and build a UI around this project. What will happen with this?  Your project will look a little nicer.  It will become a little more usable. Ok?  So in a nutshell this is the problem statement that we are targeting and we are going to build a rack system that will solve this problem statement. Ok?  Next I will discuss the exact plan of action on how we will solve this problem statement?  Now the interesting thing is that to make this project, we will use exactly the same flow that we discussed in the last video. If you remember, I showed you this diagram in my last video and I also explained step by step how a rag architecture works using this diagram.  We will solve today's problem statement on the basis of this flow and this architecture. Ok?  So let me tell you step by step how our entire flow will be for building this project. So the first thing you'll do is load the transcript of the YouTube video of which you want to chat. Ok?  So a transcript is a file where whatever is said in a YouTube video is recorded sentence by sentence at one place. Ok?  So our first goal is to somehow fetch this transcript and bring it into our project. Now this work can be done in many ways. One way you can do this is by using a document loader in Langchain which we call YT Loader.  You can load it using YouTube loader.  The second option is that YouTube also has some APIs of its own which you can hit directly to load the transcript of any video.  In our case we will be using YouTube's own API.  The reason is that I was trying to use Langchen's YouTube loader but it is a bit buggy. In case of some videos it was working properly.  The code was breaking in some videos. That is why I am going to use this YouTube API.  Ok?  So in step one, our job will be to give the ID of the video and we will load the transcript of that video. Now the transcript of the video will be quite large depending on the video.  Then what will we do?  We'll divide that transcript into multiple chunks by applying a text splitter. Once we have multiple chunks.  Then what will we do?  We will generate embeddings of all those chunks and store them in a vector store. So in this step our entire indexing work will be completed.   What we discussed is that there is step one in any rag architecture.  After that what we will do is that we will create a retriever. Ok?  For now we will use a very simple similarity search based retriever and we will send a query to that retriever. Ok?  Now this retriever will do its job.  It will embed this query and go and perform a semantic search in the vector store and fetch the relevant documents for you. Ok?  So this step is your step number two retrieval. Once you have the relevant chunks, you have your query.  Now you will merge both of them and create one form. This will be step number three which we call argumentation.  And once I get this form, I will send this form to my LLM and my LLM will understand the query.   It will understand this context and in turn will generate a response and give it to me and this will be our fourth and final step that is generation.  Ok?  So, first I will show you all this work step by step, breaking it down.  And then when this whole thing starts working, at last I will convert this whole thing into a lang chain so that automatically the output of one component becomes the input of the other component and with just one call of invoke function, this whole chain gets executed automatically. Ok?  So that is the goal for this video.  Ok?  So now that you have the big picture.  Now let's start the coding part.  So guys let me give you a step by step walk through of this code. First of all you will need your Open AI.  We will be using Open AI models to build this project.  So here's my Open AI key.  And don't worry, I have kept it trimmed.  So, if you use this exact key, then your project will not work.  So, please make sure you paste your key here. After that you have to install some libraries. You have to run this code as it is. Here we have taken all the necessary imports. And from here the main work begins.  So as I told you, we will use the same RAG architecture which I showed you in the diagram a while ago. And first of all we will complete the indexing step.  Ok? In indexing, we will first target step one, that is we will load the transcript of any YouTube video using YouTube's API and bring that as string into our code. So here I have written a code which internally uses the YouTube Transcript API.  Ok?  Like I said, there are many other ways to do this. But this method seemed most appropriate to me right now. Because it was giving me correct results with every type of videos.  For example, let's say I have this video of 3 blue one brown and I want to load the transcript of this video. So, what do you need to do to do this? First of all you will have to get the ID of this video from the URL.  So, this is your video ID.  You simply have to paste that ID here. Ok?  Please make sure you do not paste the entire URL.  I have also written here only the ID, not the full URL.  Ok?  After that we have simply written this code in the try accept block. We are calling the get transcript function from inside the YouTube transcript API. And there I have to tell you two things. First is the ID of your video. Second, which language do you want the transcript in.  Ok?  And then what happens is that the transcript that you get, let me show you by running this code once. So this transcript which you get first of all, is available on the basis of the time stamp.  That means if you print this variable once, you will see that there is a list of dictionaries where you will be shown what text is currently on the screen, at what time stamp, and for how long will it be visible on the screen.  Basically if you look here it is written the script has what the person asks the AI ​​but the AI response has been turned off.  This is a part of the complete transcript.  It will appear on the screen at a certain time and will remain on the screen for a certain duration. So you are seeing the same thing here. If I bring it up all the way. Look at this.  Right now you can see this thing on top of the video.  Now it will come after 7 seconds and will remain on the screen for 5 seconds. Ok?  Then when I play the video further, after some time the next line will be loaded.  This one supposes you also have this magical power machine, whatever it is. Hey look at this.  Ok?  This will come at 13 seconds and will remain on the screen for 3 seconds.  So basically this API is loading your transcript sentence by sentence, that is, which sentence will appear next in the subtitle after one sentence. Now what we have to do is that first we have to join all these sentences which are currently broken. So that's what I did.  I've run a loop over it and basically called the join function and we've concatenated that whole string, kind of combined it. Due to which finally you are getting a big string like this.  In which the complete transcript of your entire video is visible to you at once.  Ok?  A so this function is not as it is a function. You have to run this chunk of code as it is.  Let me show you a couple of more videos. Like this is our rag video.  If you put this here and run it. So now an error will come here.  See what error is coming?  I will read it for you. Here it shows that the video we have just chosen does not have an English transcript.  And this is a logical thing. Our videos are in Hindi. That is why English transcript is not available.  So what can you do?   You can quickly go here and write Hindi instead of English.  And now as soon as you run this code, look guys the Hindi transcript has been loaded which you will see in our video.  If you turn on captions. Ok?  A I hope you are understanding roughly.  A what I will do is I will be using the transcript of this podcast in the process of creating this project.  This is around a 2 hour podcast. So we will ask questions from this podcast later on.  Ok?  By the way, if you have not seen this video, please watch it whatever screen is on it.  It's a very very good video.  A so what I will do is I will paste it here and here I will write the language as English.  Ok ?  So now I have the complete transcript of this 2 hour video. Ok ?  And he is currently standing in this variable. Right?  So we have completed this step one. We can bring the transcript from the YouTube video into our code.  Have been able to bring it.  Ok?  Now we come to step two.  What do we need to do in step two ?  Since this is our transcript, it is the transcript of a 2 hour video. So, obviously it will be very long.  We have to divide it into small chunks. And to do this we will use a text splitter. And the text splitter we're using is called Recursive Character Text Splitter.  Ok?  I have kept the chunk size at 1000 for now. Chunk overlap I have kept 200. And I ran this code and currently I have 168 chunks created.  Ok?  So here you can experiment with different chunk sizes. Currently I was getting correct results from this particular value. So I am keeping it as ₹1200.  Ok? If you want to see a particular chunk then enter its ID in chunks and you will start seeing that chunk. Ok?  So this is like a chunk numbered 100. So at this point we have also completed step number two that we have small chunks of our entire transcript.  Ok? What we need to do in the next step is to convert all these chunks into vectors and store those vectors in a vector store.  So for that I'm going to use fi vector store. So what did we do first?  We chose an embedding model which is a model from Open AI Embeddings.  We're using this model a and we're using fires vector store. And we provided all our chunks there.  And our embedding model was provided and we ran it.  So if you see at this point, an ID has been generated against each chunk and with this ID, those chunks have been embedded and stored in our vector store. So in total there are 168 chunks.  If you want to see a particular chunk, you can quickly enter its ID in this function and check how that chunk looks.  So this is the last chunk.  Ok?  So we have completed this step also.  So basically we have completed the indexing part.  We have loaded the document.  We split the text. We embedded it and stored it in the vector store. Now the main work starts from here. In the next step we will create a retriever form and I will show you the retrieval process.  So for retrieval, first of all we have to create a retriever and we have to send a query to that retriever.  What this retriever will do is it will embed this query.   It will bring it in vector form and then go to this vector store and search which are the closest vectors to this given vector in this vector store.  And whatever chunks, whatever documents correspond to it, it will bring them to you. Ok?  So what I'm going to do is first of all I'm going to do a retriever form. And we're making a very simple retriever. We are creating our retriever using our vector store. Whose search type is similarity search.  And it will search and give you the four most similar vectors. Ok?  So we ran this code.  Here 's our retriever.  Now since the retriever itself is a runnable, it has an invoke function.  So we'll call the invoke function. And here we will ask our query. So in our case our query is what is deep mind?  But you can also ask any other question which is related to this video.  So we searched what is deep mind?  So in total, as I can see, our retriever has given four documents because we have told him that he has to give four documents. And our retriever thinks that these four documents are correct to answer this question.  Ok?  So basically our retrieval step is working fine.  This query is going here.  The retriever is searching here and we are getting these documents as output. Ok?  Always remember that the retriever gets a query as input and as output we get a list of documents. So this part is working fine.  what shall we do now?  We will go to the argumentation part.  Where do we have to do what?  We have to create a form which will merge these relevant documents and this query and then send a kind of form of that to LLM.  Ok?   The argumentation part.  So what we will do for that is first of all we will form an LLM.  However, even if I had not filled the LLM form in STEP, the work would have been done. You need LLM for the generation stage. But never the less, I completed my LLM form here itself. What are we doing here?  Creating a prep template.  The prep template is very simple. It says You are a helpful assistant answer only from the provided transcript context.  If the context is insufficient just say you don't know. Here we are giving our context. Here we are giving our question. And there are two input variables context and question.  Ok? We ran this code.  Now what are we doing here?  Once again repeating the retrieval step.  So what we are doing is we are asking a question. Is the topic of aliens discussed in this video?  If yes then what was discussed?  This is our question.  And what are we doing?  We are passing this query to retriever.invoke.  And our retriever is turning it over and giving us the retrieved documents.  If I show you what these retrieved documents look like ?  I will just create a new sale.  You can see these four documents have been returned. Ok?  So I am seeing the alien word. So now what I have to do is that I have four documents and in all these four documents there is talk about aliens. That's what our retriever is telling us. So what we will do is that we cannot send four separate documents in our prompt.  So what do we have to do?   The page content inside these four documents , basically this entire string, will have to be concatenated.  So what we are doing is we are writing a custom code where what are we doing? Going to every document.  People are picking up his page content and joining him. Basically we're creating a bigger string. Ok?  So if I show you what that big string or context looks like after this step.  So this is the output.  Ok?  A, this has been trimmed actually.  But this is a very big text.  Ok?  So now I have both things.  I already have my question. Now I also have a much bigger context.  So, I had this thing. Now this has also come.  I also converted it into string format.  Now what we can do is we can invoke prompt. And in invoke we will have to provide two things. First, we will have to give our context.  So the context is this big string that you see on the screen. And the question which we had above. Ok?  And we will call this final process.  Ok ?  So if I can show you the final plan, look like this.  You are a helpful assistant answers only from the provider in the transcript context.  A: The context starts from here.  Ok?  And finally here comes our question. So, we're going to send this whole thing that you see on the screen right now to our LLM.  Ok?  So we have completed this step also.  This part is also done with argumentation.  Now the last step remains where we will do the generation. So what do you have to do in the last step ?  Simply the LLM you created above.  Ok?  It has to be invoked.  And you have to send your final form there.  After turning around you will get an answer. If you print that answer directly then you will get something like this.  LLM is not defined.  Maybe I didn't do the above cell run.  Let me run this. And again we run this code. Ok?  So you can see yes the topic of aliens was discussed.  The Speaker expresses their personal opinion on whatever is presented. And after this I am getting a lot of other meta data. If I want just the string I will fetch the content and here is my answer.  Ok?  Here is my ans. So now this part is also working. Generation is also happening.  Ok?  You can try out different questions. Like you can ask is the topic of nuclear fuse discussed in this video?  If yes then what was discussed?  Since I have watched this video , I know these topics have been discussed.  Again we are running the same code. And we have made the final preparation.  This is our final prep and this is our generation. Yes, the topic of nuclear fuse is discussed in the video.  And all these things are explained in it.  Ok? How to control plasma etc. through AI?  There has been talk about this.  Ok?  So he told us this point by point. So that means our rag pipeline is working.  The indexing step is done , the retrieval step is done, the argumentation is done , the generation step is done.  There is only one problem.  The problem is that all these steps are working differently. Meaning we have to call every step manually. Right?  If you notice you will see it.  Firstly, we invoked the retriever separately.  Then we invoked PRPP separately and finally we invoked LLM separately as well.  Now this is not a good thing. Right?  What we can do is we can form a chain where a single time you call the invoke function and this entire pipeline will be triggered automatically. Every step will start executing automatically.  The output of each step will automatically serve as input for the next step and you will see the final result directly. Ok?  So what we are going to do next is that we are going to build a chain.  Ok?  What we learned in this playlist.  Now look, to make a chain, you will have to understand the structure of the chain once. So let's go back to our rag architecture.  Look, here you can easily understand how your chain will look.  The chain that we will make will be made by combining two chains. One of those chains is very simple.  If you focus on this chain, it is a very simple chain.  Where there is a form which you are sending to an LLM and getting a response generated from that LLM. So basically this chain will simply consist of a prop, an llm and a parser.  Ok ?  This is the simple part.  The problem is that the form here requires two inputs.  Firstly it needs the question and secondly it needs the context to answer the question.  Ok?  If you pay attention here, you will see that the form template we have created requires two inputs.  Context and Question.  Ok?  Now the question is very easy.  The question is being given to me by the user. I can send it directly to PR. Context is a bit of a tricky part.  How do you get the context that you need a retriever in the first place? You send a query to the retriever and the retriever processes the query and returns the context to you from the vector store. So this is the structure of these a vaca chains.  So this is one part of the chain which is working in a simple linear flow and then there is this part where there are actually two parallel chains.  This is a chain and this is a chain.  Ok?  So what do we have to do?  Both of these will have to be made, and both of these will have to be connected together.  Ok ?  So first of all what we will do is we will create this chain which we will call parallel chain. Because here two chains are working together. So let me show you what a parallel chain looks like. So first of all I bought some necessary imports here.  We will use Runnable Parallel.  To create a parallel chain we will use Runnable pass through and Runnable Lambda.  Ok?  Apart from this I have done one more thing that this piece of code above was just one second. This was a piece of code where we were merging return documents together to create a big string.  We're putting that inside a function.  So I created a function called formatdocs to which we will give the retrieved documents and what will this function do?  It will extract the strings from all those documents, concatenate them together and return them to you.  Ok?  Now look here the main work starts.  We are creating a chain by saying parallel chain which will be created with the help of Runnable Parallel.  Now what do you do in Runnable Parallel?  You define a dictionary and in that dictionary you define keys.  So our first key is context.  It's a question of seconds.  Now in the context one, we will put this chain in it, this chain. Ok?  So what is this chain made of?  This is made from retriever this thing and second this is made from format docs.  Ok?   The retriever is getting the query and it is returning the list of documents to us. But we cannot send the list of documents in advance.  We have to do some processing on them.  And who is doing that processing?  This function.  But it can become part of a function chain only if it is itself a runnable.  So that is why we put this function inside the runnable lambda. So now what this whole thing is exactly doing is that as soon as it gets a question, it is going to the question retriever.  The retriever performs a semantic search and retrieves a list of documents from the vector store. And we are putting that list of documents in this function. And it turns around and gives us a big string.  So basically the output of this whole chain is a big context string. Ok?  And our second chain is very simple.  That is, we are getting the question as input and we want the question itself as output.  So that is why we are using runnable pass through.  Ok ?  So I started this chain. A so what we will do is we will run all these codes once. Ok?  We have created a chain here. Now let's taste this chain.  So we're calling parallel chain dot invoke.  And we are asking this question who is Demise.  Ok ?  And we ran it.  You will notice that you will see a dictionary on the flip side. And inside this dictionary you will see two keys.  The first key is context where we have this big string which will act as our context. And the second one is the question that we originally asked.  Ok ?  So in short, we have formed this first part, this chain. We are getting two things from this chain. Our question and our context.  What do we do now? Both of these things have to be sent in this second chain. So now what we will do is we will form this chain.  So before that we are making a parser, string output parser because we want to see the output in string format.  So now what we will do is we are creating a main chain which will have three things.  Our PRt will be what we defined above.  We will have our LLM which we created above and we will have our parceler which we just created.  And we will connect this chain with our parallel chain.  So basically the two inputs coming out of the parallel chain, context and question, will go to Prapt and the one output coming out of Prap will go to LLM. LLM will generate an output which will be received by the parceler.  So this is our main chain.  Ok?  And now what we can do is we will directly call main chain dot invoke. And let's say can you summarize the video?  We can see our output in one go.  So we simplified the entire flow.  On one side we are performing indexing and on the other side with the help of this chain we are performing retrieval plus generation. Ok?  So this is a much cleaner code which you can manage further very easily. Ok?  So I hope you understood this entire code example correctly.  It would be best if you try running this code yourself once.  If possible enter the ID of any other video and try chatting with that person.  And that would be the best way to learn.  Now look, the main thing that I wanted to teach you, I have taught it to you in this video. We created a basic level rag system to solve our problem statement. Now here I want to teach you one more thing. And this is truly optional. You can also skip it if you want.  But I want to tell you how we can improve this simple rack system that we have created.  The rack systems implemented in the industry are quite complex and use a variety of techniques. So I just want to give you a little flavour of how we can improve a simple rack system to the point where it can operate at an industry grade level. Ok?  So what I will do is I will suggest you some improvements from my side.  And I have divided all these improvements into categories.  Ok? So the first category is that we can make some UI based enhancements to our Simple Rack system.  At this point, our rig is running inside of a Google CoLab notebook.  Where the user has to go manually and provide the video ID. All the sales have to be run. Then he gets the answers to his questions. Obviously a finished product, a complete product, will not work this way.  So what can we do ?  We can improve our code in such a way that the final product appears in the form of a website. Like we can use Streamlet and the user will come to this website and enter the URL of the video and by going to this website he can chat.  Right?  The second option is that you would rather build a website.  You can create a Chrome plugin like I told you about a while back.  And your user will install this Chrome plugin.  As soon as he opens YouTube, your plugin will be activated and he will watch the video here. Will chat here.  Ok?  So you can do this enhancement also.  The second improvement comes from the evaluation side.  We just learned how do you build rag based systems?  But we did not talk about evaluating it.  That means how will we know whether our system is working properly or not?  Because if you do n't know it then you can't improve it. And that is why any industry grade rack system is evaluated very carefully. Many evaluation strategies now exist.  Like there are certain libraries that do exactly this.  RAG evaluates base systems. One of the most popular libraries among them is Ragz.  Ok?  Ah now it evaluates your rag system on several metrics. Like Faithfulness.  Faithfulness means whether the answer you finally generated was related to your context or not?  Apart from this, relevance of the answer; was the answer that came out correctly related to the question or not?  Context Precision How useful was the context you retrieved with the help of the retriever in actually answering the question? Context Recall: Were we able to retrieve all the useful information that was lying in the vector store or not?  So this library measures all these things and provides them. Ok?  So if you read this library a little bit, you will get an idea of ​​how a rack based system is evaluated.  Ok? Apart from this, there is another library called Langmith Bol which you can use for training.  By placing these tracers at every step of your rack system, you can kind of check whether your entire pipeline is working properly or not.  Ok?  So evaluation is one more thing you should do if you are building an industry grade rag system.  A third is improvement in the indexing part.  Where we fetch the document, split it , embed it, and put it into the vector store.  You have a lot of scope for improvement in that part too. In this particular simple rack system.   For example, when we are doing document ingestion , we are fetching the transcript from YouTube. Now if you notice a little, the transcription that we are bringing is kind of auto generated. Meaning, there are many types of errors in it. So fixing those errors could be one thing to do.  Then I showed you that when you loaded the video of Campus x, the transcript there was in Hindi. Now, if you are getting a transcript of Hindi or a transcript of any other language, then what can you do in this particular stage?   You can first translate it into English.  So all this prepping work can be done at the document injection stage.  Similar to Text Splitting, we used Recursive Character Text Splitter.  But the problem with that is that your paragraph may get divided into two chunks in the middle of the paragraph. So here you can use the semantic chunker which I told you about in the text splitter video. Ok?  Similarly, the vector store we just used was FYERS which is a very basic level vector store.  Let’s say you’re building a rack system for a prop company. So there you will need to use a cloud based solution. So instead of using fish you can use some pine cone type solution.  Ok? So you can already see that there are so many things that you can improve.  Apart from this there are some other things, I will tell you about them. Next comes retrieval where you take your query and search for vectors related to it in the vector store. So here you work in two-three stages.  The first stage is pre- retrieval.  Just prior to retrieval. What you can do here is that you can get your user's query rewritten using an LLM.  Many times the user's query is a little short or not that meaningful.  So you can improve that query by taking LLM in between, due to which your retrieval process will also improve.  You can do multi query generation.  We read in the retrieval video that you can generate multiple queries from a single query and if your original query is a little weak then these multiple queries capture different perspectives, which improves your retrieval process a bit.  Similarly you can do domain aware routing.  So basically if you are building a very complex rack system in which you have placed multiple retrievers then what this domain aware routing will do is that depending on the query it will trigger one retriever and for another type of query it will trigger another retriever.  So you can also do the routing of the retriever.  A variety of tasks can be performed during retrieval. You can use a search strategy like MMR. Where you will get good results as well as different results from each other. You can also get hybrid retrieval done. Right now we have only done semantic search. You can do keyword search along with semantic search and merge the results of both and give it to the user.  You can also get reranking done in your results. Ok?  Currently, we are showing the most similar results to the user by arranging them on the basis of similarity score. Not showing to the user. Sending it to the next stage.  But what do you do in reranking?  With the help of an LLM, you create a new ranking of all retrieved documents and this also improves your retrieval performance.  There are many types of post retrieval work as well. One very important task among them is contextual compression.   The text that is unnecessarily appearing in your documents is not useful.   It is a little useful.  Not very useful. You will remember that photosynthesis example in the last video.  So what can you do?  By applying contextual compression you can keep only the meaningful part and remove the rest of the content. What will happen with this is that whenever you create a prep in future, there will be no wastage of space in the prep.  Ok?  So there are many kinds of optimizations possible in retrieval and we will see this later. After that comes the matter of augmentation where you design a form by combining context plus question. So here you can use prep templating very well. So the paper you are writing is to explain to LLM that look brother, this is the question and this is the context. Answer this question from this context. So you can explain this to your LLM very well.  You can explain by giving examples.  So prep templating is used a lot. Answer grounding is a very important concept.  You very explicitly tell your LLM that, look brother, whatever answer you give, give it in the context only.  Do not create answers yourself.  Do n't create facts yourself.   Do n't hallucinate.  So this is called answer grounding.  So, a lot of work is done on this as well. Ok?  And lastly you also do something called context window optimization.  So you know that LLMs can only process a certain level of tokens in the input. If you give more tokens than that then LLMs will not respond to you.  But suppose if the context you sent becomes very large then what will happen is that your context window limit will be crossed. So what you do in context window optimization is that at the time of designing the app, you trim the context that is coming from the receiver a little bit in such a way that only the useful part remains.  Remove everything else so you don't cross the context window limitations.  Ok?  So this is also a very important technique.  So this comes under argumentation.  After that finally comes the generation step. In the A Generation step, LLM basically prints out the answers for you.  So many things are done here also.  Like answer with citation.  You tell your LLM that, look brother, whenever you give any answer, you should also tell from which part of the context you gave that answer. Ok?  So we call this a citation. So you can also get citations done during generation.  After that comes the guard railing.  Guard railing is basically that you prevent your LLM from giving any wrong output, by doing this you do it.  Ok ?  So this is called guard railing.   There are many practices around it too. Again we will talk about this in a future video. But this is also a variant.  You don't want your LLM to tell anything wrong to your users. That could be very very bad.  Ok ?  So the concept to prevent this is called guard railing.  And lastly, in the entire system that you are designing, you can make different types of rack systems.  Like you can create a multimodal rack system.  So right now the rack system that we've created only works on text.  Meaning it takes in text input.  Gives text as output. But what if I want to create a rag system that can process images, can process text, can process videos as well.  So we call this type of rack system multimodal rack system. So the rack systems you will see in many companies will be multimodal rack systems.  Ok?  Apart from this you can also build agent rack.  So Agentic Rack is a rack system that does n't just operate as a chatbot. He operates as an AI agent. So, when you ask him a question, he doesn't just answer the question, but if he has to do some other work in the process of answering the question, he does that work as well. For example, you ask a question and to answer that question he not only needs a context but at the same time he also needs to go and browse the internet a bit. So what will Agentic Rack do?  It will go and connect to a web application.  It will browse there and bring those results and merge them with your context and give you the answer. So we're just adding that flavor to the rack system in the way the AI ​​agents work.  Ok? Apart from that, you can build memory based rack systems, through which you can build a little personalized rack, like, I, as a user, asked some questions today, but my rack system remembers what I talked about to it even a week ago. So he is answering me by catching the context that look, even a week ago when we talked, we had talked about this.  So you can also make memory based applications.  So on the whole I just want to tell you that whatever we read in the previous video and today's video was just the surface of the rag. Rag itself is a very big and very powerful thing.  And when you build a proper industry grade rag system you will face lot of problems and to solve those problems lot of techniques exist as I told you.  So now a completely new field has emerged in the industry whose name is Advanced Rag.  Ok? Many students were writing to me in the comments asking, Sir, when will you teach all these techniques ?  So I just want to clarify that these techniques that I have just told you in the last 10-15 minutes, we will not cover them further in the Lang Chain playlist. My plan is that once this langchen playlist is covered, I will make a separate playlist featuring advanced ragas.  There we will do all these things that I have just told you on this page.  Ok?  So super excited for that playlist as well. I hope you got a little glimpse into why advanced rack systems exist and what techniques are involved there. Ok?  Rest I hope through this video I was able to explain to you how to make a simple functional rack system. If you liked the video, please like it. If you have not subscribed to this channel , please do subscribe.  See you in the next video.  Bye.\n",
      "51\n",
      "page_content='Hi guys, my name is Nitesh and you are welcome to my YouTube channel.  In this video also we will continue our lang chain playlist. Uh in the last video, we started studying rag and we focused on discussing the theory around rag.  Did I tell you what rags are?  Why is it needed ?  I also demonstrated comparing racks there using techniques like fine tuning.  And today's video is a continuation of the previous video. Where we will practically create a rag based system using lang chain.  The plan is that I will take a problem statement and create a rig based system around that problem statement and we are going to do all this code in lang chain. So whatever you have read till now in the previous four videos, document loaders, text splitters, vector strings, we will use all these in today's video and using these, we will create a rag base system.  On the whole it's going to be a very interesting video.  Let's start.  So come on guys, first let's talk about the problem statement' metadata={'source': 'J5_-l7WIO_w'}\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Optimal for OpenAI/GPT use (adjustable)\n",
    "    chunk_overlap=200,    # Gives LLM some context from previous chunk\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"]  # Falls back gradually\n",
    ")\n",
    "\n",
    "# print(transcript[0].page_content)\n",
    "docs=text_splitter.split_documents(transcript)\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259ebe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "519804f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"mmr\", #this enables MMR\n",
    "    search_kwargs={'k':3, \"lambda_mult\":0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77fa6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant. Answer the following questions only from the provided context. If the context is not sufficient, just say \"Sorry don't have enough context to answer this question\".\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe53e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI()\n",
    "question=\"what are the possible improvements suggested in the chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d91f5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=(\n",
    "    retriever\n",
    "    | (lambda docs:{'context':\"\\n\\n\".join([doc.page_content for doc in docs]), 'question':question} )\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff302bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some possible improvements suggested in the chatbot are:\n",
      "1. Optimizing the context window to avoid crossing limitations.\n",
      "2. Implementing citation in the generation step to provide source information for answers.\n",
      "3. Using the Langmith Bol library for training.\n",
      "4. Evaluating the entire pipeline using tracers to ensure proper functioning.\n",
      "5. Improving the indexing process for fetching, splitting, embedding, and storing documents.\n",
      "6. Fixing errors in auto-generated transcripts for more accurate information.\n"
     ]
    }
   ],
   "source": [
    "question=\"what are the possible improvements suggested in the chatbot\"\n",
    "response=rag_chain.invoke(question)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Alternate way to create this chain is\n",
    "1. create a parallel chain that gives the documents fromt the retriever passed to a preprocessing function (lambdafunctions) to concatenate the docs and the other side gives the question as it is. This becomes a dictionary of context and question\n",
    "2. pass this to prompt | llm | parser\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
